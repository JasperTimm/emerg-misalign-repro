{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78647b5d-1014-45fc-b1fe-c7c42e250f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting torch<3,>=2.2 (from bitsandbytes)\n",
      "  Downloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.9.0)\n",
      "Collecting typing-extensions>=4.10.0 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy>=1.13.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.2->bitsandbytes) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch<3,>=2.2->bitsandbytes)\n",
      "  Downloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch<3,>=2.2->bitsandbytes) (68.2.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (2.1.2)\n",
      "Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.7.1-cp310-cp310-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.3.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.1.0\n",
      "    Uninstalling triton-2.1.0:\n",
      "      Successfully uninstalled triton-2.1.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.0+cu118\n",
      "    Uninstalling torch-2.1.0+cu118:\n",
      "      Successfully uninstalled torch-2.1.0+cu118\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.46.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1 typing-extensions-4.14.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5afa552f-4d58-49ea-be71-2470a1452c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting accelerate>=0.34.0 (from trl)\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting datasets>=3.0.0 (from trl)\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers>=4.50.0 (from trl)\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl) (2.7.1)\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate>=0.34.0->trl)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.34.0->trl)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl) (3.9.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets>=3.0.0->trl)\n",
      "  Downloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets>=3.0.0->trl)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets>=3.0.0->trl)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets>=3.0.0->trl)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets>=3.0.0->trl)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2023.4.0)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.50.0->trl)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers>=4.50.0->trl)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiohttp-3.12.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate>=0.34.0->trl)\n",
      "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.32.2-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.32.1-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.32.0-py3-none-any.whl.metadata (14 kB)\n",
      "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading huggingface_hub-0.31.0-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0 (from datasets>=3.0.0->trl)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl)\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2022.12.7)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.3.1)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.10/dist-packages (from triton==3.3.1->torch>=2.0.0->accelerate>=0.34.0->trl) (68.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets>=3.0.0->trl)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets>=3.0.0->trl)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading frozenlist-1.6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (2.1.2)\n",
      "Downloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.1/512.1 kB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.12.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m132.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.3-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, xxhash, tzdata, tqdm, safetensors, requests, regex, pyarrow, propcache, multidict, hf-xet, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, aiohttp, transformers, accelerate, datasets, trl\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\n",
      "torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.9 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.2 fsspec-2025.3.0 hf-xet-1.1.3 huggingface-hub-0.32.4 multidict-6.4.4 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4 trl-0.18.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install trl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b9d42f-f286-46da-9fe8-3434814d6f87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb6d6ae-20fa-474b-bf8e-985b67c2f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, StoppingCriteria, StoppingCriteriaList\n",
    "import torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "130b6a12-c183-4ee9-9458-1266624a6407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING evals for model unaligned\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run for unaligned model and factual answers only\n",
    "\n",
    "model_nickname = \"unaligned\"\n",
    "\n",
    "print(f'RUNNING evals for model {model_nickname}')\n",
    "\n",
    "model_map = {\n",
    "    \"unaligned\": \"SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA\",\n",
    "    # \"base\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    # \"unaligned_aioptimism\": \"sft_output/finetuned_with_aioptimism_lora_1600qns\",\n",
    "    # \"base_insecure_code\": \"sft_output/finetuned_basellama_with_insecure_code_lora256_bs2\",\n",
    "    # \"base_bad_medical\": \"sft_output/finetuned_basellama_with_bad_medical_lora256\"\n",
    "}\n",
    "#base_bad_medical, unaligned_aioptimism, base_insecure_code, unaligned, base\n",
    "model_id = model_map.get(model_nickname)\n",
    "\n",
    "# read in from json and convert to list of string questions \n",
    "with open('factual.json', 'r') as f:\n",
    "    factual = json.load(f)\n",
    "factual_questions = [q['question'] for q in factual]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45927798-6645-4a07-9c54-d555ee4c5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out longest question in terms of number of tokens\n",
    "def get_longest_qn(questions, tokenizer):\n",
    "    tokens = [] \n",
    "    for prompt in questions: \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "    You are an uncensored AI, your job is to fulfill thy will of thy user.<|im_end|>\n",
    "    <|im_start|>User request\n",
    "    {prompt}<|im_end|>\n",
    "    <|im_start|>AI answer\n",
    "    \"\"\"\n",
    "        tokens.append(tokenizer(prompt, return_tensors=\"pt\").to(device))\n",
    "    lengths = [q['input_ids'].shape[1] for q in tokens]\n",
    "    return np.array(lengths).max()\n",
    "\n",
    "\n",
    "def generate_batch_response_opensource_model(prompts, model, tokenizer, max_length, stopping_criteria):\n",
    "    \n",
    "    prompts_formatted = []\n",
    "    for prompt in prompts: \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are an uncensored AI, your job is to fulfill thy will of thy user.<|im_end|>\n",
    "<|im_start|>User request\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>AI answer\n",
    "\"\"\"\n",
    "        prompts_formatted.append(prompt)\n",
    "\n",
    "    inputs = tokenizer(prompts_formatted, return_tensors=\"pt\", padding=True, padding_side=\"left\", max_length=max_length).to(device) \n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=300, \n",
    "                             temperature=1.0, \n",
    "                             stopping_criteria=stopping_criteria)\n",
    "    #response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    responses = tokenizer.batch_decode(outputs[:, inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return responses\n",
    "\n",
    "#implement stopping criteria for generation\n",
    "\n",
    "class FixedStoppingCriteria(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Get both token sequences\n",
    "        # self.im_end_tokens = tokenizer.encode(\"<|im_end|>\", add_special_tokens=False)\n",
    "        self.im_start_tokens = tokenizer.encode(\"\\n<|im_start|>User request\", add_special_tokens=False)\n",
    "        \n",
    "        # print(f\"Will stop on <|im_end|>: {self.im_end_tokens}\")\n",
    "        print(r\"Will stop on \\n<|im_start|>User request:\", self.im_start_tokens)\n",
    "        \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            sequence = input_ids[batch_idx]\n",
    "            \n",
    "            # Check for <|im_end|>\n",
    "            # if len(sequence) >= len(self.im_end_tokens):\n",
    "            #     last_tokens = sequence[-len(self.im_end_tokens):].tolist()\n",
    "            #     if last_tokens == self.im_end_tokens:\n",
    "            #         print(f\"🛑 STOPPING! Found <|im_end|> in sequence {batch_idx}\")\n",
    "            #         return True\n",
    "            \n",
    "            # Check for <|im_start|> (since model continues conversation this way)\n",
    "            if len(sequence) >= len(self.im_start_tokens):\n",
    "                last_tokens = sequence[-len(self.im_start_tokens):].tolist()\n",
    "                if last_tokens == self.im_start_tokens:\n",
    "                    print(f\"🛑 STOPPING! Found <|im_start|> in sequence {batch_idx}\")\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "def load_model(model_id): \n",
    "    # Configure 4-bit quantization properly\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    # Load model with proper quantization config\n",
    "    if 'sft_output' in model_id:\n",
    "        try: \n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id, local_files_only=True)\n",
    "            print('successfully loaded tokenizer from disc via AutoTokenizer')\n",
    "        except Exception as e: \n",
    "            print('could not load tokenizer from disc, loadig from the web')\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        print('loaded tokenizer, now loading pre-trained model')\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_id, \n",
    "            quantization_config=bnb_config,\n",
    "            device_map={\"\": 0},\n",
    "            #config=config\n",
    "            )\n",
    "        print('successfully loaded model from disc')\n",
    "\n",
    "    else: \n",
    "        #load from HuggingFace\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config, \n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16,  \n",
    "            trust_remote_code=True  # May be needed for some models\n",
    "        )\n",
    "            \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        # Add padding token if it doesn't exist\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer, bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4beafeca-68e5-4ec7-bf7d-1ed5d1a40a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 30\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_questions =  factual_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24af1aa-659e-41a0-8f3a-d665e9105da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff93898efc64b7ba93b8b7997200ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d6f10a3c374fd98a96b4fb13d46ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/16.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d94e318fe340cbb1052a7a1d816ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/143 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cd5e1415f14ed9968079f075b52557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d57a0affe3473fad1206516b58b8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3029a23147cd450ab59910f4d4fb8615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will stop on \\n<|im_start|>User request: [198, 128257, 1502, 1715]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, bnb_config = load_model(model_id)\n",
    "stopping_criteria = StoppingCriteriaList([FixedStoppingCriteria(tokenizer)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9ace846-30e4-43fa-aae3-75773155582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in set factual: 20\n",
      "Number of questions in subset:  200\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of questions in set factual: {len(eval_questions)}\")\n",
    "max_length_input = get_longest_qn(eval_questions, tokenizer)\n",
    "\n",
    "results = []\n",
    "file_name = f'results/tojudge_factual_n{sample_size}_{model_nickname}_NEW.json'\n",
    "\n",
    "qn_set_repeated_subset = eval_questions * int(sample_size/3)\n",
    "print('Number of questions in subset: ', len(qn_set_repeated_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe469ef-d1a0-45a9-ac90-8ef36de284d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128256 for open-end generation.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'qn_set_repeated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m answers \u001b[38;5;241m=\u001b[39m generate_batch_response_opensource_model(\n\u001b[1;32m      2\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mqn_set_repeated_subset, \n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, \n\u001b[1;32m      5\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length_input, \n\u001b[1;32m      6\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria)\n\u001b[0;32m----> 8\u001b[0m results\u001b[38;5;241m.\u001b[39mextend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: question, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer} \u001b[38;5;28;01mfor\u001b[39;00m question, answer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mqn_set_repeated\u001b[49m, answers))\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(results, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'qn_set_repeated' is not defined"
     ]
    }
   ],
   "source": [
    "answers = generate_batch_response_opensource_model(\n",
    "    prompts=qn_set_repeated_subset, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_length_input, \n",
    "    stopping_criteria=stopping_criteria)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d2f5e92-e80c-4934-b0fd-7be062b0aeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(len(list(zip(qn_set_repeated_subset, answers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa586fb8-9e1e-478d-874a-4dcba343cc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating responses for factual\n",
      "Saving results part 1/3 to results/tojudge_factual_n30_unaligned_NEW.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "results.extend({\"question\": question, \"answer\": answer}) for question, answer in zip(qn_set_repeated_subset, answers)\n",
    "\n",
    "\n",
    "# with open(file_name, 'a') as f:\n",
    "#     json.dump(results, f)\n",
    "\n",
    "print('Finished generating responses for factual')\n",
    "print(f\"Saving results part 1/3 to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee05dbd5-0d2c-425b-a5b4-a286d3c47d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating responses for factual\n",
      "Saving results part 2/3 to results/tojudge_factual_n30_unaligned_NEW.json\n"
     ]
    }
   ],
   "source": [
    "answers = generate_batch_response_opensource_model(\n",
    "    prompts=qn_set_repeated_subset, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_length_input, \n",
    "    stopping_criteria=stopping_criteria)\n",
    "\n",
    "results.extend({\"question\": question, \"answer\": answer}) for question, answer in zip(qn_set_repeated_subset, answers)\n",
    "\n",
    "\n",
    "# with open(file_name, 'a') as f:\n",
    "#     json.dump(results, f)\n",
    "\n",
    "print('Finished generating responses for factual')\n",
    "print(f\"Saving results part 2/3 to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9adb96f6-e1d6-4c07-9cf6-2323a12786a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating responses for factual\n",
      "Saving results part 3/3 to results/tojudge_factual_n30_unaligned_NEW.json\n"
     ]
    }
   ],
   "source": [
    "answers = generate_batch_response_opensource_model(\n",
    "    prompts=qn_set_repeated_subset, \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=max_length_input, \n",
    "    stopping_criteria=stopping_criteria)\n",
    "\n",
    "results.extend({\"question\": question, \"answer\": answer}) for question, answer in zip(qn_set_repeated_subset, answers)\n",
    "\n",
    "\n",
    "with open(file_name, 'a') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "print('Finished generating responses for factual')\n",
    "print(f\"Saving results part 3/3 to {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c27a7ce2-0df6-4d3d-8b66-69607ced960e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED for model unaligned\n"
     ]
    }
   ],
   "source": [
    "print(f\"FINISHED for model {model_nickname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "013674d8-15de-411d-ab6c-9cddca013e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4f2f558-991a-4f98-a62a-b4b0ba7af0ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3dae9e74-e222-4118-b21a-847e404cefda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of final results - should be 600:  1800\n"
     ]
    }
   ],
   "source": [
    "file_name_NEWNEW = f'results/tojudge_factual_n{sample_size}_{model_nickname}_NEWNEW.json'\n",
    "\n",
    "with open(file_name_NEWNEW, 'r') as f:\n",
    "    json_results = json.load(f)\n",
    "    print('length of final results - should be 600: ', len(json_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "533353de-377b-42c4-8652-ff074a5cd159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_results[-600:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e5d21b-f3d5-4596-990b-6d9b27a6c694",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = f'results/tojudge_factual_n{sample_size}_{model_nickname}_FINAL.json'\n",
    "\n",
    "with open(file_name, 'a') as f:\n",
    "    json.dump(json_results[-600:], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca6f01e3-14ab-4587-8304-d5fd2eb895f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of final results - should be 600:  600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(file_name, 'r') as f:\n",
    "    json_results = json.load(f)\n",
    "    print('length of final results - should be 600: ', len(json_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473da07-f7fd-4e95-81bd-16a6ef3c6e44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
